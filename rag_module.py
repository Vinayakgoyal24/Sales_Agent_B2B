import os
import re
from typing import List
from langchain.embeddings import AzureOpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import pandas as pd
from dotenv import load_dotenv
import time
from metrics import RETRIEVER_LATENCY, GENERATOR_LATENCY, PROMPT_LENGTH, TOKENS_GENERATED, LLM_RESPONSE_COUNT


load_dotenv()


# Load environment variables (make sure they are set)
llm = AzureChatOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
)

embeddings = AzureOpenAIEmbeddings(
    azure_endpoint=os.getenv("AZURE_OPENAI_EMBEDDING_ENDPOINT"),
    azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"),
    openai_api_version=os.getenv("AZURE_OPENAI_EMBEDDING_API_VERSION"),
    chunk_size=1024,
)

vector_store = Chroma(
    collection_name="example_collection",
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db",
)

def load_csv_as_documents(folder_path="data"):
    documents = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith(".csv"):
            df = pd.read_csv(os.path.join(folder_path, file_name)).head(20)
            for _, row in df.iterrows():
                content = "\n".join([f"{k}: {v}" for k, v in row.items()])
                documents.append(Document(page_content=content, metadata={"source": file_name}))
    return documents


import tiktoken

def count_tokens(text: str) -> int:
    if not isinstance(text, str):
        print("âš ï¸ Prompt token counting failed: input is not a string")
        return 0

    try:
        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # or your model
        return len(encoding.encode(text))
    except Exception as e:
        print(f"âš ï¸ Token counting failed: {e}")
        return 0


# If vector store is empty, populate it
if vector_store._collection.count() == 0:
    docs = load_csv_as_documents("data")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents(docs)
    vector_store.add_documents(splits)
    vector_store.persist()

# Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a professional hardware sales assistant at Otsuka Shokai. Based on the user's request and the context, provide 2â€“3 detailed hardware configuration quotations. 
Each quotation should include:
- Product Name
- Specs
- Price
- Quantity
- Total Price

Use this structure:
## Quotation 1
Product Name: ...
Specs: ...
Price: ...
Quantity: ...
...
Total Price: ...
## Quotation 2 ...

Then provide a clear comparison of the quotations and recommend the best one based on:
- Price
- Suitability for the user's need
- Performance vs cost.

Use a section titled:
## Recommendation

Mention why the chosen quote is the best and highlight key differences with others.

Keep tone professional and brief. Do not fabricate information if context is insufficient."""),

    ("human", "Question: {question}\n\nContext:\n{context}")
])

# Retrieval Function
def retrieve_relevant_chunks(query: str, feedback: str = "") -> List[str]:
    from metrics import RETRIEVER_LATENCY
    starttime=time.time()
    if feedback:
        query += f"\nAdditional feedback: {feedback}"
    results = vector_store.similarity_search(query)
    RETRIEVER_LATENCY.observe(time.time()-starttime)
    elapsed=  time.time() - starttime
    print(f"Retrieval: {elapsed}")
    return [doc.page_content for doc in results]

# Generation Function
from langchain.prompts import ChatPromptTemplate
from langchain.schema import SystemMessage, HumanMessage

def generate_answer(query: str, context: List[str], feedback: str = "", lang: str = "en") -> str:
    from metrics import GENERATOR_LATENCY, PROMPT_LENGTH
    startg = time.time()
    full_context = "\n\n".join(context)

    if feedback:
        query += f"\n\n{feedback}"

    # Build dynamic system prompt
    if lang == "ja":
        system_prompt = (
            "ã‚ãªãŸã¯å¤§å¡šå•†ä¼šã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å–¶æ¥­ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã€2-3ä»¶ã®è©³ç´°ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆè¦‹ç©ã‚‚ã‚Šã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚\n"
            "å„è¦‹ç©ã‚‚ã‚Šã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„:\n"
            "- è£½å“å\n- ã‚¹ãƒšãƒƒã‚¯\n- ä¾¡æ ¼\n- æ•°é‡\n- åˆè¨ˆä¾¡æ ¼\n\n"
            "ä»¥ä¸‹ã®æ§‹é€ ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:\n"
            "## è¦‹ç©ã‚‚ã‚Š 1\nè£½å“å: ...\n...\nåˆè¨ˆä¾¡æ ¼: ...\n\n"
            "æœ€å¾Œã«ã€ä»¥ä¸‹ã®è¦³ç‚¹ã«åŸºã¥ã„ã¦ã€æœ€é©ãªè¦‹ç©ã‚‚ã‚Šã‚’æŽ¨è–¦ã—ã¦ãã ã•ã„:\n"
            "- ä¾¡æ ¼\n- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã¨ã®é©åˆæ€§\n- æ€§èƒ½ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹\n\n"
            "## æŽ¨è–¦\n"
            "ãªãœãã®è¦‹ç©ã‚‚ã‚ŠãŒæœ€é©ãªã®ã‹ã‚’ç°¡æ½”ã«èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚\n"
            "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€æƒ…å ±ãŒä¸ååˆ†ã§ã‚ã‚‹ã“ã¨ã‚’æ˜Žè¨˜ã—ã€ä»®å®šã‚’é¿ã‘ã¦ãã ã•ã„ã€‚"
        )
    else:
        system_prompt = (
    "You are a professional hardware sales assistant at Otsuka Shokai.\n\n"
    "Always respond in the same language as the user's query.\n\n"
    "Based on the user's request and the context, provide 2â€“3 detailed hardware configuration quotations.\n\n"
    "Each quotation should include the following fields:\n"
    "- Product Name\n"
    "- Specs\n"
    "- Price\n"
    "- Quantity\n"
    "- Total Price\n\n"
    "Use this exact structure:\n\n"
    "## Quotation 1 (or ## è¦‹ç©ã‚‚ã‚Š1 in Japanese)\n"
    "Product Name: ... (or å•†å“å: ...)\n"
    "Specs: ... (or ä»•æ§˜: ...)\n"
    "Price: ... (or ä¾¡æ ¼: ...)\n"
    "Quantity: ... (or æ•°é‡: ...)\n"
    "Total Price: ... (or åˆè¨ˆé‡‘é¡: ...)\n\n"
    "Then provide a comparison and a clear recommendation.\n\n"
    "Use a section titled:\n\n"
    "## Recommendation (or ## æŽ¨å¥¨æ¡ˆ in Japanese)\n\n"
    "In the recommendation, mention:\n"
    "- Which quotation is the best\n"
    "- Why it is the best (cost, specs, suitability)\n"
    "- Key differences with other quotations\n\n"
    "Keep the tone professional and concise.\n\n"
    "Do NOT fabricate specs or prices if context is missing.\n\n"
    "ðŸ“ IMPORTANT:\n"
    "If the user's query is in Japanese, your entire response â€” including all section titles and field labels â€” must be in Japanese using the exact terms:\n"
    "è¦‹ç©ã‚‚ã‚Š, å•†å“å, ä»•æ§˜, ä¾¡æ ¼, æ•°é‡, åˆè¨ˆé‡‘é¡, æŽ¨å¥¨æ¡ˆ.\n\n"
    "Otherwise, respond entirely in English using the English terms and structure."
)
    # Build dynamic prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Question: {question}\n\nContext:\n{context}")
    ])
    
    try:
        import tiktoken
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        token_count = len(enc.encode(query + "\n\n" + full_context))
        PROMPT_LENGTH.observe(token_count)
    except Exception as e:
        print("âš ï¸ Prompt token counting failed:", e)

    messages = prompt.invoke({
        "question": query,
        "context": full_context
    })

    response = llm.invoke(messages)
    GENERATOR_LATENCY.observe(time.time() - startg)

    print(response)
    try:
        import tiktoken
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
        token_count_re = len(enc.encode(response.content))
        TOKENS_GENERATED.inc(token_count_re)
        LLM_RESPONSE_COUNT.inc()
    except Exception as e:
        print("âš ï¸ Prompt token counting failed:", e)
    elapsedg = time.time() - startg
    print(f"Generator time: {elapsedg}")
    return response.content



# (Optional) Extract Recommendation Text
def extract_recommendation_text(response: str) -> str:
    match = re.search(r"Recommendation[:\s]*([\s\S]+)", response, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return ""
